
## training setup
nWorkers: 4 ## number of workers that play games
nGames: 10000 ## number of games that each worker will play.

## hyperparameters
updateFrequency: 50000 ## parameters are updated every 'updateFrequency' steps
rewardDiscount: 0.99 ## discount factor for future rewards
lambda: 0.96 ## lambda value for generalized advantage estimation
normRange: 200000 # how many episodes are used to normalize the reward and advantage
normalizeRewards: True
normalizeAdvantages: True

## optimizer setup
optimizer: 'rmsprop'
optimizerArgs:
  learning_rate: 0.001
  gamma1: 0.9
  gamma2: 0
  centered: True
  clip_gradient :

## organisation
outputDir: 'test/pong/v1_reward_advantage_normalization_long_incentive'       ## if None, no output is saved. Otherwise, the model and log are saved every saveInterval games.
saveInterval: 100                       ## model will be saved after saveInterval episodes.
verbose: False                         ## whether to store extended log with gradient and parameter info
trainerFile:  #'C:/Users/markus_2/Documents/Nerding/python/a3c/test/pong/python3_gae_ppo_longGameIncentive_discrete_episode20_continued5_oneBallEpisode/1700/trainer.states' ## file with initial states for the trainer 