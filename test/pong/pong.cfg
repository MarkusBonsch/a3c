
## training setup
nWorkers: 4 ## number of workers that play games
nGames: 5000 ## number of games that each worker will play.

## hyperparameters
updateFrequency: 50000 ## parameters are updated every 'updateFrequency' steps
rewardDiscount: 0.99 ## discount factor for future rewards
lambda: 0.96 ## lambda value for generalized advantage estimation
normRange: 200000 # how many episodes are used to normalize the reward and advantage
normalizeRewards: False # If True Rewards are normalized before advantage calculation
normalizeAdvantages: True # if True, advantages are normalized
updateOnPartDone: True # if True, env.isPartDone triggers an update (e.g. one ball in pong). If False, only env.isDone and updateSteps can trigger an update.

## optimizer setup
optimizer: 'rmsprop'
optimizerArgs:
  learning_rate: 0.001
  gamma1: 0.9
  gamma2: 0
  centered: True
  clip_gradient :

## organisation
outputDir:  'test/pong/v2_Episode20_UnnormalizedRewards'      ## The model and log are saved every saveInterval games.
saveInterval: 100                      ## model will be saved after saveInterval episodes.
verbose: False                         ## whether to store extended log with gradient and parameter info
trainerFile:  #'C:/Users/markus_2/Documents/Nerding/python/a3c/test/pong/python3_gae_ppo_longGameIncentive_discrete_episode20_continued5_oneBallEpisode/1700/trainer.states' ## file with initial states for the trainer 