
## training setup
nWorkers: 4 ## number of workers that play games
nGames: 4000 ## number of games that each worker will play.

## hyperparameters
updateFrequency: 50000 ## parameters are updated every 'updateFrequency' steps
rewardDiscount: 0.01 ## discount factor for future rewards
useGAE: True # if True, generalized Advantage estimation is used. If false, advantage is just the discounted sum of rewards - value.
lambda: 0.96 # lambda value for generalized advantage estimation
normRange: 200000 # how many episodes are used to normalize the reward and advantage
normalizeRewards: False # If True Rewards are normalized before advantage calculation
normalizeAdvantages: False # if True, advantages are normalized
updateOnPartDone: False # if True, env.isPartDone triggers an update (e.g. one ball in pong). If False, only env.isDone and updateSteps can trigger an update.

## optimizer setup
optimizer: 'rmsprop'
optimizerArgs:
  learning_rate: 0.001
  gamma1: 0.9
  gamma2: 0
  centered: True
  clip_gradient :

## organisation
outputDir:  'C:/users/markus_2/Documents/Nerding/python/a3c/test/dinner_simple/v8_testFreeSeatSelectorLayer'      ## The model and log are saved every saveInterval games.
saveInterval: 1000                      ## model will be saved after saveInterval episodes.
verbose: False                         ## whether to store extended log with gradient and parameter info
trainerFile:  #'C:/Users/markus_2/Documents/Nerding/python/a3c/test/dinner_simple/v1_9teams_pad30/final/trainer.states' ## file with initial states for the trainer 